---
title: "Linear-Regression"
author: "Rahul Sinha"
date: 2018-11-23
output:
  html_document:
    toc: yes
  pdf_document:
    toc: yes
---

a. Generate a vector X with 30 values, where each value is generated from a normal distribution with mean 30 and standard deviation 5. Generate a vector Y with 30 values where each value is 10 times the corresponding value in X plus a random value, generated from a normal distribution with mean 3 and standard deviation 1. Now, fit a linear model.


```{r}
# Generate X and Y
X <- rnorm(30, mean = 30, sd = 5)
Y <- ((10 * X) + rnorm(30, mean = 3, sd = 1))
```

```{r}
# Fit a linear model
model1 <- lm(Y ~ X)
summary(model1)
```
```{r}
# test if residuals are normally distributed: yes
shapiro.test(residuals(model1))
```

```{r}
# 1. What are the coefficients?
model1$coefficients
cat("Intercept Coefficient: ", model1$coefficients[1], "\n")
cat("Regression Coefficient: ", model1$coefficients[2], "\n")
```
```{r}
# test for Homoscedasticity: yes
library(lmtest)
bptest(model1)
```

```{r}
# 2. What do you predict the Y value will be if X = 32.8?
x1 <- 32.8
y1 <- (model1$coefficients[1] + (model1$coefficients[2]) * x1)
y1
```

```{r}
# 3. What is the R2 value?
ssr_a = sum((fitted(model1) - mean(Y))^2)
sse_a = sum((fitted(model1) - Y)^2)
sst_a = sse_a + ssr_a

r2_a = ssr_a / sst_a
r2_a
```

b. Generate a vector X with 30 values, where each value is generated from a uniform distribution with minimum 20 and maximum 30. Generate a vector Y with 30 values where each value is 10 times the corresponding value in X plus a random value, generated from a normal distribution with mean 3 and standard deviation 1. Now, fit a linear model.

```{r}
# Generate X and Y
Xb <- runif(30, min = 20, max = 30)
Yb <- ((10 * Xb) + rnorm(30, mean = 3, sd = 1))
```

```{r}
# Fit a linear model
model_1b <- lm(Yb ~ Xb)
summary(model_1b)
```

```{r}
shapiro.test(residuals(model_1b))
```

```{r}
library(lmtest)
bptest(model_1b)
```

```{r}
# 1. What are the coefficients?
model_1b$coefficients
cat("Intercept Coefficient: ", model_1b$coefficients[1], "\n")
cat("Regression Coefficient: ", model_1b$coefficients[2], "\n")
```


```{r}
# 2. What do you predict the Y value will be if X = 32.8?
xb <- 32.8
yb <- (model_1b$coefficients[1] + (model_1b$coefficients[2]) * xb)
yb
```

```{r}
# 3. What is the R2 value?
ssr_b = sum((fitted(model_1b) - mean(Yb))^2)
sse_b = sum((fitted(model_1b) - Yb)^2)
sst_b = sse_b + ssr_b

r2_b = ssr_b / sst_b
r2_b
```


c. Generate a vector X with 30 values, where each value is generated from a normal distribution with mean 30 and standard deviation 5. Generate a vector Y with 30 values where each value is 10 times the corresponding value in X squared, plus a random value, generated from a normal distribution with mean 3 and standard deviation 1. Now, fit a linear model.
```{r}
# Generate X and Y
Xc <- rnorm(30, mean = 30, sd = 5)
Yc <- ((10 * (Xc^2)) + rnorm(30, mean = 3, sd = 1))
```

```{r}
# Fit a linear model
model_1c <- lm(Yc ~ Xc)
summary(model_1c)
```
```{r}
shapiro.test(residuals(model_1c))
```

```{r}
library(lmtest)
bptest(model_1c)
```

```{r}
# 1. What are the coefficients?
model_1c$coefficients
cat("Intercept Coefficient: ", model_1c$coefficients[1], "\n")
cat("Regression Coefficient: ", model_1c$coefficients[2], "\n")
```

```{r}
# 2. What do you predict the Y value will be if X = 32.8?
x1c <- 32.8
y1c <- (model_1c$coefficients[1] + (model_1c$coefficients[2]) * x1c)
y1c
```

```{r}
# 3. What is the R2 value?
ssr_c = sum((fitted(model_1c) - mean(Yc))^2)
sse_c = sum((fitted(model_1c) - Yc)^2)
sst_c = sse_c + ssr_c

r2_c = ssr_c / sst_c
r2_c
```

d. Compare the three models. Why do you think the R2 values compare as they do? Generating scatter plots may be useful.
```{r}
r2_a
r2_b
r2_c

coefficients(model1)
coefficients(model_1b)
coefficients(model_1c)
```

```{r}
# Plots of X, Y, and our best fit line
plot(X, Y)
abline(model1)
plot(Xb, Yb)
abline(model_1b)
plot(Xc, Yc)
abline(model_1c)

# The R squared value is very high for all three models as the Y values are tightly wrapped around the best fit line predicted by our Regression Analysis. 
# We understand that our values of Y are generated by a function of X. Thus, we expect a very high coefficient of determination or R squared.
```

# Linear Regression
Consider the data set ServiceTimes.xlsx, which contains 60 randomly chosen service times for customers at two different branches of a bank, 30 from each.

a. Formulate hypotheses for testing the differences between the mean service time of the two branches.
```{r}
# Read data from ServiceTimes.xlsx 
library(readxl)
ServiceTime <- read_excel("Data/ServiceTimes.xlsx")
#View(ServiceTime)
```

```{r}
# Formulate Hypothesis
# Null Hypothesis: Difference between the mean service times for the two branches is 0.
# Alternative Hypothesis: Difference between the mean service times for the two branches is not 0.
```

b. Is there statistically significant differences between the mean services times between the two branches?
```{r}
# we check if the values are normally distributed for the two variables
shapiro.test(ServiceTime$branch_1)
shapiro.test(ServiceTime$branch_2)

# we find that values in Service Time values in branch_1 aren't normally distributed. We thus go for a bootstarp test.
```
```{r}
library(wBoot)
boot.two.per(ServiceTime$branch_1,
             ServiceTime$branch_2,
             mean, 
             stacked = FALSE,
             null.hyp = 0
             )
# As per the boot test, the p-value = 0.0247
# which is less than the significance level of 0.05, we thus reject the Null Hypothesis
# that the Difference between the mean service times for the two branches is 0 
# at a significance level of 0.05 with p-value of 0.0247.
```


Consider the data set mtcars which is available in base R.
a. Build a linear regression model predicting mpg using hp.

```{r}
attach(mtcars)
#str(mtcars)
model3 <- lm(mpg ~ hp, data = mtcars)
summary(model3)
```

b. What is the regression equation?
```{r}
# regression equation:
# mpg = 30.09886 + (-0.06823 * hp)

```

c. What would you predict the mpg of a car to be if hp is 110 using the regression equation?
```{r}
hp_x = 110
mpg_x = 30.09886 + (-0.06823 * hp_x)
mpg_x
```

d. Run a statistical test to determine if there is a linear relationship between hp and mpg.
Formally write the null and alternative hypotheses. What is the p-value of the test?

Null Hypothesis (Ho): There is no linear relationship between hp and mpg. That is, if we were to say that, mpg = beta0 + (beta1 * hp), then beta1 is 0.

Alternate Hypothesis (H1): There is linear relationship between hp and mpg. mpg = beta0 + (beta1 * hp), then beta1 is not 0.


```{r}
# Print the summary of the t-test done by using the lm function
model3 <- lm(mpg ~ hp, data = mtcars)
summary(model3)

# we note that the Null Hypothesis is that there is no linear relationship between hp and mpg. beta1=0.
# as per the lm function's test, the test-statistic = -6.742, and the 
# corressponding p-value = 1.79 x 10^(-7), which is lower than our signifiance level of 0.05.
# We hence reject the Null Hypothesis and conclude that beta1 is not 0 at a significance level of 5% and that there is a linear relationship between mpg and hp.

# p-value: 1.788e-07

# We can also look up the values of estimate and standard error and t value to calculate the pvalue by hand and accordingly test the hypothesis
# p-value
2*(pt(-6.742, df=30))

# test statistic for the t test
-0.06823 / 0.01012

# Thus, we reject the null hypothesis at the significance level: 0.05

# we can also run bootstrap with the same null hypothesis and find that the p-value is smaller than our significance level of 0.05,hence reject null hypothesis.
# additionally we get the observed slope value(beta1) of -0.068
boot.slope.per(hp, mpg, null.hyp = 0,
               conf.level = 0.95, 
               R = 9999
               )
```

e. As briefly mentioned in class, there is a statistical test that can be run for the correlation between two variables. If you have variable X and variable Y , the null hypothesis is that there is no correlation between the two variables, and the alternative hypothesis is that there is non-zero correlation between the two variables. The command is cor.test which takes two arguments, that are the variables you are testing for correlation. Before running the test, what is the correlation between hp and mpg?

```{r}
# bootstrap to check correlation
boot.cor.per(hp, mpg, null.hyp = 0, conf.level = 0.95, type = NULL, R = 9999)

# p-value: P < 0.001. Therefore, we reject the Null Hypothesis that there is no correlation between the two variables hp and mpg, at significance level of 0.05
# bootstrap gives a correlation coefficient of -0.7761684
```


f. Run a statistical test to determine if there is correlation between the two variables using cor.test(mtcars$mpg,mtcars$hp). This has various outputs. What is the confidence interval? Note: This represents a 95% confidence interval for the correlation between the two variables.
```{r}
# We run the cor.test which ahs the following hypotheses:
# Null Hypothesis: correlation between the two variables is 0.
# Alternate Hypothesis: correlation is not equal to 0

cor.test(mtcars$mpg,mtcars$hp)
# Confidence interval (95% confidence interval for the correlation between the two variables)
# lower: -0.8852686 
# upper: -0.5860994

# p-value: 1.788e-07
```

g. What is the p-value for the statistical test in the previous part? Do you reject the null hypothesis?
```{r}
# p-value: 1.788e-07
# the p-value is extremely low for the 5% significance level.
# we reject the null hypothesis that there is no correlation between the two variables - hp and mpg at 0.05 significance level and a p-value of 1.788e-07.
```

h. In part d. you found a p-value for whether or not there is a statistically significant linear relationship between hp and mpg, and you found a p-value. How does that p-value compare with the p-value found in part g.? Explain the relationship.
```{r}
# part d
# p-value: 1.788e-07

# part g
# p-value: 1.788e-07

# The two p-values are equal.

# In Part d, the Null Hypothesis is that there is no linear relationship between hp and mpg, i.e. beta1 is 0.
# In parts f and g, while testing for correlation, our Null Hypothesis: correlation between the two variables is 0.
# in linear regression, if Null Hypothesis is true, we are essentially stating that the covariance between the two variables, response and the predictor, doesn't have any predictive power.
# in correlation test too, if Null Hypothesis is true, we are essentially stating that the covariance between two variables isn't different from 0, i.e. it doesn't have any predictive power.
# Hence, we get the same p-values for the above tests as they both have essentially the same Null Hypotheses- the covariance of the two variables is 0, i.e. it doesn't have any predictive power.
# This is evidenced by the formulas for beta1 and correlation coefficient.
# beta1 = cov(X,Y)/var(X)
# correlation coefficient = cov(X,Y) / (sd(X) * sd(Y))
# both are 0 when cov(X,Y) is 0.
```


